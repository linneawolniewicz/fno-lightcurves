{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with FNO\n",
    "Train an FNO timeseries classifier on the FordA dataset from the UCR/UEA archive.\n",
    "\n",
    "Much of this comes from the following Keras tutorial: https://keras.io/examples/timeseries/timeseries_classification_from_scratch/\n",
    "\n",
    "The dataset we are using here is called FordA. The data comes from the UCR archive. The dataset contains 3601 training instances and another 1320 testing instances. Each timeseries corresponds to a measurement of engine noise captured by a motor sensor. For this task, the goal is to automatically detect the presence of a specific issue with the engine. The problem is a balanced binary classification task. The full description of this dataset can be found here: http://www.j-wichard.de/publications/FordPaper.pdf\n",
    "\n",
    "Later, can include the features mentioned in the paper; namely the autocorrelation values and spectral density features as separate channels, akin to the work we will do later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linneamw/sadow_koastore/personal/linneamw/anaconda3/envs/fno/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR, ExponentialLR\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from lightning import LightningModule, Trainer\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor\n",
    "import datetime\n",
    "import optuna\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape:  (3601, 1, 500)\n",
      "x_test shape:  (1320, 1, 500)\n",
      "Number of classes: 2\n",
      "Training set size: 2880\n",
      "Validation set size: 721\n"
     ]
    }
   ],
   "source": [
    "# Read the data\n",
    "def readucr(filename):\n",
    "    data = np.loadtxt(filename, delimiter=\"\\t\")\n",
    "    y = data[:, 0]\n",
    "    x = data[:, 1:]\n",
    "    return x, y.astype(int)\n",
    "\n",
    "root_url = \"https://raw.githubusercontent.com/hfawaz/cd-diagram/master/FordA/\"\n",
    "\n",
    "x_train, y_train = readucr(root_url + \"FordA_TRAIN.tsv\")\n",
    "x_test, y_test = readucr(root_url + \"FordA_TEST.tsv\")\n",
    "\n",
    "# Reshape the data to be ready for multivariate time-series data (multiple channels)\n",
    "# Shape is (samples, channels, sequence length)\n",
    "x_train = x_train.reshape((x_train.shape[0], 1, x_train.shape[1]))\n",
    "x_test = x_test.reshape((x_test.shape[0], 1, x_test.shape[1]))\n",
    "print(\"x_train shape: \", x_train.shape)\n",
    "print(\"x_test shape: \", x_test.shape)\n",
    "\n",
    "# Count the number of classes\n",
    "num_classes = len(np.unique(y_train))\n",
    "print(\"Number of classes: \" + str(num_classes))\n",
    "\n",
    "# Standardize the labels to positive integers. The expected labels will then be 0 and 1.\n",
    "y_train[y_train == -1] = 0\n",
    "y_test[y_test == -1] = 0\n",
    "\n",
    "# Use 20% of training data for validation\n",
    "train_set_size = int(len(x_train) * 0.8)\n",
    "valid_set_size = len(x_train) - train_set_size\n",
    "print(\"Training set size: \" + str(train_set_size))\n",
    "print(\"Validation set size: \" + str(valid_set_size))\n",
    "\n",
    "# split the x_train and y_train set into two\n",
    "seed = torch.Generator().manual_seed(42)\n",
    "x_train, x_valid = data.random_split(x_train, [train_set_size, valid_set_size], generator=seed)\n",
    "y_train, y_valid = data.random_split(y_train, [train_set_size, valid_set_size], generator=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample batch of data (batch size, # channels, sequence length): torch.Size([32, 1, 500])\n"
     ]
    }
   ],
   "source": [
    "# Create train, valid, and test data loaders with float64 datatype\n",
    "batch_size = 32\n",
    "workers = 1\n",
    "train_loader = DataLoader(\n",
    "    list(zip(x_train, y_train)),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=workers,\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    list(zip(x_valid, y_valid)),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=True,\n",
    "    num_workers=workers,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    list(zip(x_test, y_test)),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=True,\n",
    "    num_workers=workers,\n",
    ")\n",
    "\n",
    "# Print the size of a batch and type of data\n",
    "for x, y in train_loader:\n",
    "    print(\"Sample batch of data (batch size, # channels, sequence length): \" + str(x.shape))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FNO Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: use neuralop package\n",
    "# from neuralop.models.spectral_convolution import FactorizedSpectralConv\n",
    "# fourier_layer = FactorizedSpectralConv(in_channels=in_channels, out_channels=out_channels, n_modes=(modes1))\n",
    "\n",
    "# Option 2: create my own spectral convolution layer\n",
    "class SpectralConv1d(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,   # Number of input channels\n",
    "                 out_channels,  # Number of output channels\n",
    "                 modes,         # Number of Fourier modes to multiply, at most floor(N/2) + 1\n",
    "                 debug=False):  # If true, print the shape of every parameter      \n",
    "        super(SpectralConv1d, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.modes = modes\n",
    "        self.debug = debug\n",
    "\n",
    "        self.scale = (1 / (in_channels * out_channels))\n",
    "        self.weights = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes, dtype=torch.cfloat))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.shape[0]\n",
    "\n",
    "        #Compute Fourier coefficients\n",
    "        x_ft = torch.fft.rfft(x)\n",
    "\n",
    "        # Multiply relevant Fourier modes\n",
    "        out_ft = torch.zeros(batchsize, self.out_channels,  x.size(-1)//2 + 1, dtype=torch.cfloat, device=x.device)\n",
    "        \n",
    "        if self.debug:\n",
    "            print(\"x shape: \" + str(x.shape))\n",
    "            print(\"x_ft.shape: \" + str(x_ft.shape))\n",
    "            print(\"out_ft.shape: \" + str(out_ft.shape))\n",
    "            print(\"self.weights.shape: \" + str(self.weights.shape))\n",
    "\n",
    "        out_ft[:, :, :self.modes] = \\\n",
    "            self.compl_mul1d(x_ft[:, :, :self.modes], self.weights)\n",
    "\n",
    "        #Return to physical space\n",
    "        x = torch.fft.irfft(out_ft)\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"irfft out_ft shape: \" + str(x.shape) + \"\\n\")\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def compl_mul1d(self, input, weights):\n",
    "        # (batch, in_channel, x), (in_channel, out_channel, x) -> (batch, out_channel, x)\n",
    "        return torch.einsum(\"bix,iox->box\", input, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNOClassifier(LightningModule):\n",
    "    def __init__(self, \n",
    "                 modes, \n",
    "                 lr=1e-3, \n",
    "                 channels=[64, 64, 64], \n",
    "                 pooling=500, \n",
    "                 optimizer=\"adam\", \n",
    "                 scheduler=\"reducelronplateau\", \n",
    "                 momentum=0.9,\n",
    "                 pool_type=\"max\"\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lr = lr\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.momentum = momentum\n",
    "        self.num_channels = len(channels)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.example_input_array = torch.rand(1, 1, 500)\n",
    "\n",
    "        for i in range(self.num_channels):\n",
    "            if i == 0:\n",
    "                in_channels = 1\n",
    "            else:\n",
    "                in_channels = channels[i-1]\n",
    "\n",
    "            out_channels = channels[i]\n",
    "\n",
    "            setattr(self, f\"fno_layer_{i}\", nn.Sequential(\n",
    "                SpectralConv1d(in_channels, out_channels, modes),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            ))\n",
    "        \n",
    "        if pool_type == \"max\":\n",
    "            self.pool = nn.MaxPool1d(pooling)\n",
    "        else:\n",
    "            self.pool = nn.AvgPool1d(pooling)\n",
    "\n",
    "        self.fc = nn.Linear(channels[-1] * int((500/pooling)), 2) # output number of channels of final fno_block * (3rd input dimension 500 / maxpool size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(self.num_channels):\n",
    "            x = getattr(self, f\"fno_layer_{i}\")(x)\n",
    "            x = F.tanh(x)\n",
    "\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x    \n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = self.forward(x)\n",
    "\n",
    "        # Log the loss\n",
    "        loss = self.loss(x, y) # No need for softmax, as it is included in nn.CrossEntropyLoss\n",
    "        self.log(\"train_loss\", loss)\n",
    "\n",
    "        # Log the accuracy\n",
    "        logits = F.softmax(x, dim=1)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = (preds == y).float().mean()\n",
    "        self.log(\"train_acc\", acc)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = self.forward(x)\n",
    "\n",
    "        # Log the loss\n",
    "        loss = self.loss(x, y)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        \n",
    "        # Log the accuracy\n",
    "        logits = F.softmax(x, dim=1)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = (preds == y).float().mean()\n",
    "        self.log(\"val_acc\", acc)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = self.forward(x)\n",
    "\n",
    "        # Log the loss\n",
    "        loss = self.loss(x, y)\n",
    "        self.log(\"test_loss\", loss)\n",
    "\n",
    "        # Log the accuracy\n",
    "        logits = F.softmax(x, dim=1)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = (preds == y).float().mean()\n",
    "        self.log(\"test_acc\", acc)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        if self.optimizer == \"sgd\":\n",
    "            optimizer = torch.optim.SGD(self.parameters(), lr=self.lr, momentum=self.momentum)\n",
    "        else:\n",
    "            optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "        if self.scheduler == \"reducelronplateau\":\n",
    "            scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=5, min_lr=1e-6)\n",
    "        elif self.scheduler == \"cosineannealinglr\":\n",
    "            scheduler = CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-6)\n",
    "        else:\n",
    "            scheduler = ExponentialLR(optimizer, gamma=0.95)\n",
    "        \n",
    "        return [optimizer], [{\"scheduler\": scheduler, \"monitor\": \"val_loss\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FNOClassifier(\n",
      "  (loss): CrossEntropyLoss()\n",
      "  (fno_layer_0): Sequential(\n",
      "    (0): SpectralConv1d()\n",
      "    (1): BatchNorm1d(8192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (pool): AvgPool1d(kernel_size=(500,), stride=(500,), padding=(0,))\n",
      "  (fc): Linear(in_features=8192, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "modes = 25 \n",
    "channels = [8192] \n",
    "pool_type = \"avg\" \n",
    "pooling = 500\n",
    "\n",
    "# Optimizers and learning rate schedulers\n",
    "optimizer = \"adam\"\n",
    "momentum = 0 \n",
    "scheduler = \"cosineannealinglr\"\n",
    "lr = 1e-3\n",
    "\n",
    "# Initialize classifier\n",
    "classifier = FNOClassifier(modes=modes, lr=lr, channels=channels, pooling=pooling, optimizer=optimizer, scheduler=scheduler, momentum=momentum, pool_type=pool_type)\n",
    "\n",
    "# Print the model\n",
    "print(classifier.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorboard logs will be saved to: logs/fitting_train_trying_to_generalize/2024_03_04-16_52_20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params | In sizes       | Out sizes     \n",
      "-----------------------------------------------------------------------------------\n",
      "0 | loss        | CrossEntropyLoss | 0      | ?              | ?             \n",
      "1 | fno_layer_0 | Sequential       | 221 K  | [1, 1, 500]    | [1, 8192, 500]\n",
      "2 | pool        | AvgPool1d        | 0      | [1, 8192, 500] | [1, 8192, 1]  \n",
      "3 | fc          | Linear           | 16.4 K | [1, 8192]      | [1, 2]        \n",
      "-----------------------------------------------------------------------------------\n",
      "237 K     Trainable params\n",
      "0         Non-trainable params\n",
      "237 K     Total params\n",
      "0.950     Total estimated model params size (MB)\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:  27%|██▋       | 24/90 [00:01<00:03, 17.41it/s, v_num=2_20]       "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linneamw/sadow_koastore/personal/linneamw/anaconda3/envs/fno/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "# Create a learning rate scheduler and early stopping callback\n",
    "callbacks = [\n",
    "    # EarlyStopping(monitor=\"val_acc\", patience=30, mode=\"max\"),\n",
    "    LearningRateMonitor(logging_interval=\"step\")\n",
    "]\n",
    "\n",
    "# Create a tensorboard logger\n",
    "experiment_name = \"fitting_train_trying_to_generalize\"\n",
    "save_directory = \"logs/\"\n",
    "\n",
    "# Check if save_dir/experiment_name exists, if not create it\n",
    "if not os.path.exists(save_directory + experiment_name):\n",
    "    os.makedirs(save_directory + experiment_name)\n",
    "\n",
    "logger = TensorBoardLogger(save_dir=save_directory, name=experiment_name, version=datetime.datetime.now().strftime(\"%Y_%m_%d-%H_%M_%S\"))\n",
    "logger.log_hyperparams({\n",
    "    \"modes\": modes, \n",
    "    \"lr\": lr, \n",
    "    \"channels\": channels,\n",
    "    \"pool_type\": pool_type, \n",
    "    \"pooling\": pooling, \n",
    "    \"lr_scheduler\": scheduler, \n",
    "    \"batchsize\": batch_size, \n",
    "    \"optimizer\": optimizer, \n",
    "    \"momentum\": momentum\n",
    "})\n",
    "print(\"Tensorboard logs will be saved to: \" + logger.log_dir)\n",
    "\n",
    "# Train the model\n",
    "trainer = Trainer(max_epochs=500,\n",
    "                  logger=logger,\n",
    "                  callbacks=callbacks,\n",
    "                  accelerator=\"auto\",\n",
    "                  log_every_n_steps=10,\n",
    "                #   overfit_batches=1\n",
    ")\n",
    "\n",
    "trainer.fit(model=classifier, \n",
    "            train_dataloaders=train_loader,\n",
    "            val_dataloaders=valid_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "trainer.test(dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predictions\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "for x, y in test_loader:\n",
    "    x = x.to(classifier.device)\n",
    "    y = y.to(classifier.device)\n",
    "    logits = F.softmax(classifier(x), dim=1)\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    y_pred.extend(logits.cpu().detach().numpy())\n",
    "    y_true.extend(y.cpu().detach().numpy())\n",
    "\n",
    "# Compute the ROC curve and AUC\n",
    "fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color=\"darkorange\", lw=2, label=\"ROC curve (area = %0.2f)\" % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver Operating Characteristic\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "\n",
    "# Save the figure to the logs directory\n",
    "plt.savefig(logger.log_dir + \"/roc_curve.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create an optuna study to optimize the number of modes, channels, and pooling size\n",
    "# def objective(trial):\n",
    "#     # Optimize the number of modes\n",
    "#     modes = trial.suggest_int(\"modes\", 5, 15)\n",
    "\n",
    "#     # Optimize the number of fno_layers\n",
    "#     fno_layers = trial.suggest_int(\"fno_layers\", 2, 5)\n",
    "\n",
    "#     # Optimize the number of channels\n",
    "#     channels = [trial.suggest_int(f\"n_channels_{i}\", 16, 256) for i in range(fno_layers)]\n",
    "\n",
    "#     # Optimize the pooling size\n",
    "#     pooling = trial.suggest_int(\"pooling\", 2, 500)\n",
    "\n",
    "#     # Optimize the learning rate\n",
    "#     lr = trial.suggest_float(\"lr\", 1e-4, 1e-1, log=True)\n",
    "\n",
    "#     # Create the model\n",
    "#     model = FNOClassifier(modes=modes, channels=channels, pooling=pooling)\n",
    "\n",
    "#     # Create a learning rate scheduler and early stopping callback\n",
    "#     callbacks = [\n",
    "#         EarlyStopping(monitor=\"val_acc\", patience=20, mode=\"max\"),\n",
    "#         LearningRateMonitor(logging_interval=\"step\")\n",
    "#     ]\n",
    "\n",
    "#     # Create a tensorboard logger\n",
    "#     experiment_name = \"optuna\"\n",
    "#     logger = TensorBoardLogger(save_dir=\"logs/\", name=experiment_name, version=datetime.datetime.now().strftime(\"%Y_%m_%d-%H_%M_%S\"))\n",
    "\n",
    "#     logger.log_hyperparams({\"modes\": modes, \"lr\": lr, \"channels\": channels, \"pooling\": pooling, \"lr_scheduler\": \"ReduceLROnPlateau\", \"patience\": 10, \"min_lr\": 1e-6, \"factor\": 0.5, \"batchsize\": batch_size})\n",
    "\n",
    "#     # Create a trainer\n",
    "#     trainer = Trainer(\n",
    "#         max_epochs=100,\n",
    "#         logger=logger,\n",
    "#         callbacks=callbacks,\n",
    "#         accelerator='auto'\n",
    "#     )\n",
    "\n",
    "#     # Train the model\n",
    "#     trainer.fit(model, train_loader, valid_loader)\n",
    "\n",
    "#     # Test the model\n",
    "#     result = trainer.test(dataloaders=test_loader)\n",
    "\n",
    "#     # Return the validation accuracy\n",
    "#     return result[0][\"test_acc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# study = optuna.create_study(direction=\"maximize\")\n",
    "# study.optimize(objective, n_trials=500)\n",
    "\n",
    "# # Print the best hyperparameters\n",
    "# print(study.best_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fno",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
