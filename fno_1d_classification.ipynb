{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with FNO\n",
    "Train an FNO timeseries classifier on the FordA dataset from the UCR/UEA archive.\n",
    "\n",
    "Much of this comes from the following Keras tutorial: https://keras.io/examples/timeseries/timeseries_classification_from_scratch/\n",
    "\n",
    "The dataset we are using here is called FordA. The data comes from the UCR archive. The dataset contains 3601 training instances and another 1320 testing instances. Each timeseries corresponds to a measurement of engine noise captured by a motor sensor. For this task, the goal is to automatically detect the presence of a specific issue with the engine. The problem is a balanced binary classification task. The full description of this dataset can be found here: http://www.j-wichard.de/publications/FordPaper.pdf\n",
    "\n",
    "Later, can include the features mentioned in the paper; namely the autocorrelation values and spectral density features as separate channels, akin to the work we will do later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR, ExponentialLR\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torchaudio\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, RichProgressBar\n",
    "import datetime\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.4'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pytorch_lightning\n",
    "pytorch_lightning.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape:  (3601, 1, 500)\n",
      "x_test shape:  (1320, 1, 500)\n",
      "Number of classes: 2\n",
      "Training set size: 2880\n",
      "Validation set size: 721\n"
     ]
    }
   ],
   "source": [
    "# Read the data\n",
    "def readucr(filename):\n",
    "    data = np.loadtxt(filename, delimiter=\"\\t\")\n",
    "    y = data[:, 0]\n",
    "    x = data[:, 1:]\n",
    "    return x, y.astype(int)\n",
    "\n",
    "root_url = \"https://raw.githubusercontent.com/hfawaz/cd-diagram/master/FordA/\"\n",
    "\n",
    "x_train, y_train = readucr(root_url + \"FordA_TRAIN.tsv\")\n",
    "x_test, y_test = readucr(root_url + \"FordA_TEST.tsv\")\n",
    "\n",
    "# Reshape the data to be ready for multivariate time-series data (multiple channels)\n",
    "# Shape is (samples, channels, sequence length)\n",
    "x_train = x_train.reshape((x_train.shape[0], 1, x_train.shape[1]))\n",
    "x_test = x_test.reshape((x_test.shape[0], 1, x_test.shape[1]))\n",
    "print(\"x_train shape: \", x_train.shape)\n",
    "print(\"x_test shape: \", x_test.shape)\n",
    "\n",
    "# Scale the data to be between 0 and 1\n",
    "min_val = min(np.min(x_train), np.min(x_test))\n",
    "max_val = max(np.max(x_train), np.max(x_test))\n",
    "x_train = (x_train - min_val) / (max_val - min_val)\n",
    "x_test = (x_test - min_val) / (max_val - min_val)\n",
    "\n",
    "# Count the number of classes\n",
    "num_classes = len(np.unique(y_train))\n",
    "print(\"Number of classes: \" + str(num_classes))\n",
    "\n",
    "# Standardize the labels to positive integers. The expected labels will then be 0 and 1.\n",
    "y_train[y_train == -1] = 0\n",
    "y_test[y_test == -1] = 0\n",
    "\n",
    "# Use 20% of training data for validation\n",
    "train_set_size = int(len(x_train) * 0.8)\n",
    "valid_set_size = len(x_train) - train_set_size\n",
    "print(\"Training set size: \" + str(train_set_size))\n",
    "print(\"Validation set size: \" + str(valid_set_size))\n",
    "\n",
    "# split the x_train and y_train set into two\n",
    "seed = torch.Generator().manual_seed(42)\n",
    "x_train, x_valid = data.random_split(x_train, [train_set_size, valid_set_size], generator=seed)\n",
    "y_train, y_valid = data.random_split(y_train, [train_set_size, valid_set_size], generator=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom Dataset class for your data\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x_data, y_data, transform=None):\n",
    "        self.x_data = x_data\n",
    "        self.y_data = y_data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.x_data[index]\n",
    "        y = self.y_data[index]\n",
    "\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample batch of data (batch size, # channels, sequence length): torch.Size([128, 1, 500])\n"
     ]
    }
   ],
   "source": [
    "# Create train, valid, and test data loaders with float64 datatype\n",
    "batch_size = 128\n",
    "workers = 0\n",
    "train_loader = DataLoader(\n",
    "    CustomDataset(x_train, y_train),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=workers,\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    CustomDataset(x_valid, y_valid),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=True,\n",
    "    num_workers=workers,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    CustomDataset(x_test, y_test),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=True,\n",
    "    num_workers=workers,\n",
    ")\n",
    "\n",
    "# Print the size of a batch and type of data\n",
    "for x, y in train_loader:\n",
    "    print(\"Sample batch of data (batch size, # channels, sequence length): \" + str(x.shape))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FNO Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: use neuralop package\n",
    "# from neuralop.models.spectral_convolution import FactorizedSpectralConv\n",
    "# fourier_layer = FactorizedSpectralConv(in_channels=in_channels, out_channels=out_channels, n_modes=(modes1))\n",
    "\n",
    "# Option 2: create my own spectral convolution layer\n",
    "class SpectralConv1d(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,   # Number of input channels\n",
    "                 out_channels,  # Number of output channels\n",
    "                 modes,         # Number of Fourier modes to multiply, at most floor(N/2) + 1\n",
    "                 debug=False):  # If true, print the shape of every parameter      \n",
    "        super(SpectralConv1d, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.modes = modes\n",
    "        self.debug = debug\n",
    "\n",
    "        self.scale = (1 / (in_channels * out_channels))\n",
    "        self.weights = torch.empty(in_channels, out_channels, self.modes, dtype=torch.cfloat)\n",
    "        nn.init.xavier_normal_(self.weights, gain=1.0)\n",
    "        self.weights = nn.Parameter(self.weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.shape[0]\n",
    "\n",
    "        #Compute Fourier coefficients\n",
    "        x_ft = torch.fft.rfft(x)\n",
    "\n",
    "        # Multiply relevant Fourier modes\n",
    "        out_ft = torch.zeros(batchsize, self.out_channels,  x.size(-1)//2 + 1, dtype=torch.cfloat, device=x.device)\n",
    "        \n",
    "        if self.debug:\n",
    "            print(\"x shape: \" + str(x.shape))\n",
    "            print(\"x_ft.shape: \" + str(x_ft.shape))\n",
    "            print(\"out_ft.shape: \" + str(out_ft.shape))\n",
    "            print(\"self.weights.shape: \" + str(self.weights.shape))\n",
    "\n",
    "        out_ft[:, :, :self.modes] = \\\n",
    "            self.compl_mul1d(x_ft[:, :, :self.modes], self.weights)\n",
    "\n",
    "        #Return to physical space\n",
    "        x = torch.fft.irfft(out_ft)\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"irfft out_ft shape: \" + str(x.shape) + \"\\n\")\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def compl_mul1d(self, input, weights):\n",
    "        # (batch, in_channel, x), (in_channel, out_channel, x) -> (batch, out_channel, x)\n",
    "        return torch.einsum(\"bix,iox->box\", input, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNOClassifier(LightningModule):\n",
    "    def __init__(self, \n",
    "                 modes, \n",
    "                 lr=1e-3, \n",
    "                 channels=[64, 64, 64], \n",
    "                 pooling=500, \n",
    "                 optimizer=\"adam\", \n",
    "                 scheduler=\"reducelronplateau\", \n",
    "                 momentum=0.9,\n",
    "                 pool_type=\"max\"\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lr = lr\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.momentum = momentum\n",
    "        self.num_channels = len(channels)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.example_input_array = torch.rand(1, 1, 500)\n",
    "\n",
    "        for i in range(self.num_channels):\n",
    "            if i == 0:\n",
    "                in_channels = 1\n",
    "            else:\n",
    "                in_channels = channels[i-1]\n",
    "\n",
    "            out_channels = channels[i]\n",
    "\n",
    "            setattr(self, f\"fno_layer_{i}\", nn.Sequential(\n",
    "                SpectralConv1d(in_channels, out_channels, modes),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            ))\n",
    "        \n",
    "        if pool_type == \"max\":\n",
    "            self.pool = nn.MaxPool1d(pooling)\n",
    "        else:\n",
    "            self.pool = nn.AvgPool1d(pooling)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        self.fc = nn.Linear(channels[-1] * int((500/pooling)), 2) # output number of channels of final fno_block * (3rd input dimension 500 / maxpool size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(self.num_channels):\n",
    "            x = getattr(self, f\"fno_layer_{i}\")(x)\n",
    "            x = F.tanh(x)\n",
    "\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # noise = torch.randn_like(x)  # adding noise doesn't seem to help\n",
    "        # x = x + noise\n",
    "        x = self.dropout(x) # dropping out seems to help a little, but not to the extent we need it to \n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x    \n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = self.forward(x)\n",
    "\n",
    "        # Log the loss\n",
    "        loss = self.loss(x, y) # No need for softmax, as it is included in nn.CrossEntropyLoss\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        # Log the accuracy\n",
    "        logits = F.softmax(x, dim=1)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = (preds == y).float().mean()\n",
    "        self.log(\"train_acc\", acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        # collapse flag \n",
    "        collapse_flg = torch.unique(logits).size(dim=0)\n",
    "        self.log(\"collapse_flg_train\", collapse_flg, sync_dist=True, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = self.forward(x)\n",
    "\n",
    "        # Log the loss\n",
    "        loss = self.loss(x, y)\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        # Log the accuracy\n",
    "        logits = F.softmax(x, dim=1)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = (preds == y).float().mean()\n",
    "        self.log(\"val_acc\", acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        # collapse flag \n",
    "        collapse_flg = torch.unique(logits).size(dim=0)\n",
    "        self.log(\"collapse_flg\", collapse_flg, sync_dist=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = self.forward(x)\n",
    "\n",
    "        # Log the loss\n",
    "        loss = self.loss(x, y)\n",
    "        self.log(\"test_loss\", loss)\n",
    "\n",
    "        # Log the accuracy\n",
    "        logits = F.softmax(x, dim=1)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = (preds == y).float().mean()\n",
    "        self.log(\"test_acc\", acc)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        if self.optimizer == \"sgd\":\n",
    "            optimizer = torch.optim.SGD(self.parameters(), lr=self.lr, momentum=self.momentum)\n",
    "        else:\n",
    "            optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "        if self.scheduler == \"reducelronplateau\":\n",
    "            scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=5, min_lr=1e-6)\n",
    "        elif self.scheduler == \"cosineannealinglr\":\n",
    "            scheduler = CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-6)\n",
    "        else:\n",
    "            scheduler = ExponentialLR(optimizer, gamma=0.95)\n",
    "        \n",
    "        return [optimizer], [{\"scheduler\": scheduler, \"monitor\": \"val_loss\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "modes = 250\n",
    "channels = [8092] \n",
    "pool_type = \"avg\" \n",
    "pooling = 500\n",
    "\n",
    "# Optimizers and learning rate schedulers\n",
    "optimizer = \"adam\"\n",
    "momentum = 0\n",
    "scheduler = \"cosineannealinglr\"\n",
    "lr = 1e-3\n",
    "\n",
    "# Initialize classifier\n",
    "classifier = FNOClassifier(modes=modes, lr=lr, channels=channels, pooling=pooling, optimizer=optimizer, scheduler=scheduler, momentum=momentum, pool_type=pool_type)\n",
    "\n",
    "# Print the model\n",
    "# print(classifier.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [6]\n",
      "/mnt/srl-oahu-1/srl-hawaii-1/ariannab/anaconda3/envs/torch_fno/lib/python3.10/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "\n",
      "  | Name        | Type             | Params | In sizes       | Out sizes     \n",
      "-----------------------------------------------------------------------------------\n",
      "0 | loss        | CrossEntropyLoss | 0      | ?              | ?             \n",
      "1 | fno_layer_0 | Sequential       | 2.0 M  | [1, 1, 500]    | [1, 8092, 500]\n",
      "2 | pool        | AvgPool1d        | 0      | [1, 8092, 500] | [1, 8092, 1]  \n",
      "3 | dropout     | Dropout          | 0      | [1, 8092]      | [1, 8092]     \n",
      "4 | fc          | Linear           | 16.2 K | [1, 8092]      | [1, 2]        \n",
      "-----------------------------------------------------------------------------------\n",
      "2.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.1 M     Total params\n",
      "8.221     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 23.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/srl-oahu-1/srl-hawaii-1/ariannab/anaconda3/envs/torch_fno/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=255` in the `DataLoader` to improve performance.\n",
      "/mnt/srl-oahu-1/srl-hawaii-1/ariannab/anaconda3/envs/torch_fno/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=255` in the `DataLoader` to improve performance.\n",
      "/mnt/srl-oahu-1/srl-hawaii-1/ariannab/anaconda3/envs/torch_fno/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:293: The number of training batches (22) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22:  73%|███████▎  | 16/22 [00:01<00:00,  9.70it/s, v_num=41, train_loss_step=0.322, train_acc_step=0.898, val_loss=2.680, val_acc=0.539, collapse_flg=1.600, train_loss_epoch=0.369, train_acc_epoch=0.862, collapse_flg_train=2.000]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/srl-oahu-1/srl-hawaii-1/ariannab/anaconda3/envs/torch_fno/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "# Create a learning rate scheduler and early stopping callback\n",
    "callbacks = [\n",
    "    # EarlyStopping(monitor=\"val_acc\", patience=30, mode=\"max\"),\n",
    "    LearningRateMonitor(logging_interval=\"epoch\")\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "trainer = Trainer(max_epochs=500,\n",
    "                  callbacks=callbacks,\n",
    "                  accelerator=\"auto\"\n",
    ")\n",
    "\n",
    "trainer.fit(model=classifier, \n",
    "            train_dataloaders=train_loader,\n",
    "            val_dataloaders=valid_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "\n",
    "trainer.test(dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the predictions\n",
    "# y_pred = []\n",
    "# y_true = []\n",
    "\n",
    "# for x, y in test_loader:\n",
    "#     x = x.to(classifier.device)\n",
    "#     y = y.to(classifier.device)\n",
    "#     logits = F.softmax(classifier(x), dim=1)\n",
    "#     preds = torch.argmax(logits, dim=1)\n",
    "#     y_pred.extend(logits.cpu().detach().numpy())\n",
    "#     y_true.extend(y.cpu().detach().numpy())\n",
    "\n",
    "# # Compute the ROC curve and AUC\n",
    "# fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "# roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# # Plot the ROC curve\n",
    "# plt.figure()\n",
    "# plt.plot(fpr, tpr, color=\"darkorange\", lw=2, label=\"ROC curve (area = %0.2f)\" % roc_auc)\n",
    "# plt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.xlabel(\"False Positive Rate\")\n",
    "# plt.ylabel(\"True Positive Rate\")\n",
    "# plt.title(\"Receiver Operating Characteristic\")\n",
    "# plt.legend(loc=\"lower right\")\n",
    "\n",
    "\n",
    "# # Save the figure to the logs directory\n",
    "# plt.savefig(logger.log_dir + \"/roc_curve.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create an optuna study to optimize the number of modes, channels, and pooling size\n",
    "# def objective(trial):\n",
    "#     # Optimize the number of modes\n",
    "#     modes = trial.suggest_int(\"modes\", 5, 15)\n",
    "\n",
    "#     # Optimize the number of fno_layers\n",
    "#     fno_layers = trial.suggest_int(\"fno_layers\", 2, 5)\n",
    "\n",
    "#     # Optimize the number of channels\n",
    "#     channels = [trial.suggest_int(f\"n_channels_{i}\", 16, 256) for i in range(fno_layers)]\n",
    "\n",
    "#     # Optimize the pooling size\n",
    "#     pooling = trial.suggest_int(\"pooling\", 2, 500)\n",
    "\n",
    "#     # Optimize the learning rate\n",
    "#     lr = trial.suggest_float(\"lr\", 1e-4, 1e-1, log=True)\n",
    "\n",
    "#     # Create the model\n",
    "#     model = FNOClassifier(modes=modes, channels=channels, pooling=pooling)\n",
    "\n",
    "#     # Create a learning rate scheduler and early stopping callback\n",
    "#     callbacks = [\n",
    "#         EarlyStopping(monitor=\"val_acc\", patience=20, mode=\"max\"),\n",
    "#         LearningRateMonitor(logging_interval=\"step\")\n",
    "#     ]\n",
    "\n",
    "#     # Create a tensorboard logger\n",
    "#     experiment_name = \"optuna\"\n",
    "#     logger = TensorBoardLogger(save_dir=\"logs/\", name=experiment_name, version=datetime.datetime.now().strftime(\"%Y_%m_%d-%H_%M_%S\"))\n",
    "\n",
    "#     logger.log_hyperparams({\"modes\": modes, \"lr\": lr, \"channels\": channels, \"pooling\": pooling, \"lr_scheduler\": \"ReduceLROnPlateau\", \"patience\": 10, \"min_lr\": 1e-6, \"factor\": 0.5, \"batchsize\": batch_size})\n",
    "\n",
    "#     # Create a trainer\n",
    "#     trainer = Trainer(\n",
    "#         max_epochs=100,\n",
    "#         logger=logger,\n",
    "#         callbacks=callbacks,\n",
    "#         accelerator='auto'\n",
    "#     )\n",
    "\n",
    "#     # Train the model\n",
    "#     trainer.fit(model, train_loader, valid_loader)\n",
    "\n",
    "#     # Test the model\n",
    "#     result = trainer.test(dataloaders=test_loader)\n",
    "\n",
    "#     # Return the validation accuracy\n",
    "#     return result[0][\"test_acc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# study = optuna.create_study(direction=\"maximize\")\n",
    "# study.optimize(objective, n_trials=500)\n",
    "\n",
    "# # Print the best hyperparameters\n",
    "# print(study.best_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fno_torch",
   "language": "python",
   "name": "fno_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
