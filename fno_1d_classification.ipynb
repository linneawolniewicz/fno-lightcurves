{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with FNO\n",
    "Train an FNO timeseries classifier on the FordA dataset from the UCR/UEA archive.\n",
    "\n",
    "Much of this comes from the following Keras tutorial: https://keras.io/examples/timeseries/timeseries_classification_from_scratch/\n",
    "\n",
    "The dataset we are using here is called FordA. The data comes from the UCR archive. The dataset contains 3601 training instances and another 1320 testing instances. Each timeseries corresponds to a measurement of engine noise captured by a motor sensor. For this task, the goal is to automatically detect the presence of a specific issue with the engine. The problem is a balanced binary classification task. The full description of this dataset can be found here: http://www.j-wichard.de/publications/FordPaper.pdf\n",
    "\n",
    "Later, can include the features mentioned in the paper; namely the autocorrelation values and spectral density features as separate channels, akin to the work we will do later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.model_utils import FNOClassifier\n",
    "from utils.data_utils import CustomDataset\n",
    "\n",
    "import os\n",
    "import optuna\n",
    "import datetime\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from lightning import LightningModule, Trainer\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor, RichProgressBar\n",
    "pl.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available on device NVIDIA GeForce RTX 2070 with device count: 1\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"GPU is available on device {torch.cuda.get_device_name(0)} with device count: {torch.cuda.device_count()}\")\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape:  (3601, 1, 500)\n",
      "x_test shape:  (1320, 1, 500)\n",
      "Number of classes: 2\n",
      "Training set size: 2880\n",
      "Validation set size: 721\n"
     ]
    }
   ],
   "source": [
    "# Read the data\n",
    "def readucr(filename):\n",
    "    data = np.loadtxt(filename, delimiter=\"\\t\")\n",
    "    y = data[:, 0]\n",
    "    x = data[:, 1:]\n",
    "    return x, y.astype(int)\n",
    "\n",
    "root_url = \"https://raw.githubusercontent.com/hfawaz/cd-diagram/master/FordA/\"\n",
    "\n",
    "x_train, y_train = readucr(root_url + \"FordA_TRAIN.tsv\")\n",
    "x_test, y_test = readucr(root_url + \"FordA_TEST.tsv\")\n",
    "\n",
    "# Reshape the data to be ready for multivariate time-series data (multiple channels)\n",
    "# Shape is (samples, channels, sequence length)\n",
    "x_train = x_train.reshape((x_train.shape[0], 1, x_train.shape[1]))\n",
    "x_test = x_test.reshape((x_test.shape[0], 1, x_test.shape[1]))\n",
    "print(\"x_train shape: \", x_train.shape)\n",
    "print(\"x_test shape: \", x_test.shape)\n",
    "\n",
    "# Scale the data to be between 0 and 1\n",
    "min_val = min(np.min(x_train), np.min(x_test))\n",
    "max_val = max(np.max(x_train), np.max(x_test))\n",
    "x_train = (x_train - min_val) / (max_val - min_val)\n",
    "x_test = (x_test - min_val) / (max_val - min_val)\n",
    "\n",
    "# Count the number of classes\n",
    "num_classes = len(np.unique(y_train))\n",
    "print(\"Number of classes: \" + str(num_classes))\n",
    "\n",
    "# Standardize the labels to positive integers. The expected labels will then be 0 and 1.\n",
    "y_train[y_train == -1] = 0\n",
    "y_test[y_test == -1] = 0\n",
    "\n",
    "# Use 20% of training data for validation\n",
    "train_set_size = int(len(x_train) * 0.8)\n",
    "valid_set_size = len(x_train) - train_set_size\n",
    "print(\"Training set size: \" + str(train_set_size))\n",
    "print(\"Validation set size: \" + str(valid_set_size))\n",
    "\n",
    "# split the x_train and y_train set into two\n",
    "seed = torch.Generator().manual_seed(42)\n",
    "x_train, x_valid = data.random_split(x_train, [train_set_size, valid_set_size], generator=seed)\n",
    "y_train, y_valid = data.random_split(y_train, [train_set_size, valid_set_size], generator=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample batch of data (batch size, # channels, sequence length): torch.Size([32, 1, 500])\n"
     ]
    }
   ],
   "source": [
    "# Create train, valid, and test data loaders\n",
    "batch_size = 32\n",
    "workers = 0\n",
    "train_loader = DataLoader(\n",
    "    CustomDataset(x_train, y_train),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=workers,\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    CustomDataset(x_valid, y_valid),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=True,\n",
    "    num_workers=workers,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    CustomDataset(x_test, y_test),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=True,\n",
    "    num_workers=workers,\n",
    ")\n",
    "\n",
    "# Print the size of a batch and type of data\n",
    "for x, y in train_loader:\n",
    "    print(\"Sample batch of data (batch size, # channels, sequence length): \" + str(x.shape))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FNOClassifier(\n",
      "  (loss): CrossEntropyLoss()\n",
      "  (fno_layer_0): Sequential(\n",
      "    (0): SpectralConv1d()\n",
      "    (1): BatchNorm1d(8192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (pool): AvgPool1d(kernel_size=(500,), stride=(500,), padding=(0,))\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=8192, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "modes = 25\n",
    "channels = [8192] \n",
    "pool_type = \"avg\" \n",
    "pooling = 500\n",
    "p_dropout = 0.5\n",
    "add_noise = True\n",
    "\n",
    "# Optimizers and learning rate schedulers\n",
    "# lr schedule options are reducelronplateau, cosineannealinglr, cosineannealingwarmrestarts, and linearwarmupcosineannealingwarmrestarts\n",
    "optimizer = \"adam\"\n",
    "momentum = 0 \n",
    "scheduler = \"linearwarmupcosineannealingwarmrestarts\"\n",
    "lr = 1e-3 # note if scheduler is linearwarmupcosineannealingwarmrestarts this has no effect\n",
    "\n",
    "# Initialize classifier\n",
    "classifier = FNOClassifier(\n",
    "                modes=modes, \n",
    "                lr=lr, \n",
    "                channels=channels, \n",
    "                pooling=pooling, \n",
    "                optimizer=optimizer, \n",
    "                scheduler=scheduler, \n",
    "                momentum=momentum, \n",
    "                pool_type=pool_type, \n",
    "                p_dropout=p_dropout, \n",
    "                add_noise=add_noise\n",
    ")\n",
    "\n",
    "# Print the model\n",
    "print(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorboard logs will be saved to: logs/fitting_train_trying_to_generalize/2024_03_08-03_37_25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params | In sizes        | Out sizes      \n",
      "-------------------------------------------------------------------------------------\n",
      "0 | loss        | CrossEntropyLoss | 0      | ?               | ?              \n",
      "1 | fno_layer_0 | Sequential       | 221 K  | [16, 1, 500]    | [16, 8192, 500]\n",
      "2 | pool        | AvgPool1d        | 0      | [16, 8192, 500] | [16, 8192, 1]  \n",
      "3 | dropout     | Dropout          | 0      | [16, 8192]      | [16, 8192]     \n",
      "4 | fc          | Linear           | 16.4 K | [16, 8192]      | [16, 2]        \n",
      "-------------------------------------------------------------------------------------\n",
      "237 K     Trainable params\n",
      "0         Non-trainable params\n",
      "237 K     Total params\n",
      "0.950     Total estimated model params size (MB)\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ed46e720300495a8fec9af3010194cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f284e17d8f8421695cc794a66473266",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "768140dbdff749a2ab6d1386c7f6456f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a tensorboard logger\n",
    "experiment_name = \"fitting_train_trying_to_generalize\"\n",
    "save_directory = \"logs/\"\n",
    "\n",
    "# Check if save_dir/experiment_name exists, if not create it\n",
    "if not os.path.exists(save_directory + experiment_name):\n",
    "    os.makedirs(save_directory + experiment_name)\n",
    "\n",
    "logger = TensorBoardLogger(save_dir=save_directory, name=experiment_name, version=datetime.datetime.now().strftime(\"%Y_%m_%d-%H_%M_%S\"))\n",
    "logger.log_hyperparams({\n",
    "    \"modes\": modes, \n",
    "    \"lr\": lr, \n",
    "    \"channels\": channels,\n",
    "    \"pool_type\": pool_type, \n",
    "    \"pooling\": pooling, \n",
    "    \"lr_scheduler\": scheduler, \n",
    "    \"batchsize\": batch_size, \n",
    "    \"optimizer\": optimizer, \n",
    "    \"momentum\": momentum,\n",
    "    \"p_dropout\": p_dropout,\n",
    "    \"add_noise\": add_noise\n",
    "})\n",
    "print(\"Tensorboard logs will be saved to: \" + logger.log_dir)\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=30, mode=\"min\"),\n",
    "    LearningRateMonitor(logging_interval=\"step\"),\n",
    "    # RichProgressBar(leave=True)\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "trainer = Trainer(max_epochs=500,\n",
    "                  logger=logger,\n",
    "                  callbacks=callbacks,\n",
    "                  accelerator=\"auto\",\n",
    "                #   overfit_batches=1\n",
    ")\n",
    "\n",
    "trainer.fit(model=classifier, \n",
    "            train_dataloaders=train_loader,\n",
    "            val_dataloaders=valid_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "\n",
    "trainer.test(dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the predictions\n",
    "# y_pred = []\n",
    "# y_true = []\n",
    "\n",
    "# for x, y in test_loader:\n",
    "#     x = x.to(classifier.device)\n",
    "#     y = y.to(classifier.device)\n",
    "#     logits = F.softmax(classifier(x), dim=1)\n",
    "#     preds = torch.argmax(logits, dim=1)\n",
    "#     y_pred.extend(logits.cpu().detach().numpy())\n",
    "#     y_true.extend(y.cpu().detach().numpy())\n",
    "\n",
    "# # Compute the ROC curve and AUC\n",
    "# fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "# roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# # Plot the ROC curve\n",
    "# plt.figure()\n",
    "# plt.plot(fpr, tpr, color=\"darkorange\", lw=2, label=\"ROC curve (area = %0.2f)\" % roc_auc)\n",
    "# plt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.xlabel(\"False Positive Rate\")\n",
    "# plt.ylabel(\"True Positive Rate\")\n",
    "# plt.title(\"Receiver Operating Characteristic\")\n",
    "# plt.legend(loc=\"lower right\")\n",
    "\n",
    "\n",
    "# # Save the figure to the logs directory\n",
    "# plt.savefig(logger.log_dir + \"/roc_curve.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create an optuna study to optimize the number of modes, channels, and pooling size\n",
    "# def objective(trial):\n",
    "#     # Optimize the number of modes\n",
    "#     modes = trial.suggest_int(\"modes\", 5, 15)\n",
    "\n",
    "#     # Optimize the number of fno_layers\n",
    "#     fno_layers = trial.suggest_int(\"fno_layers\", 2, 5)\n",
    "\n",
    "#     # Optimize the number of channels\n",
    "#     channels = [trial.suggest_int(f\"n_channels_{i}\", 16, 256) for i in range(fno_layers)]\n",
    "\n",
    "#     # Optimize the pooling size\n",
    "#     pooling = trial.suggest_int(\"pooling\", 2, 500)\n",
    "\n",
    "#     # Optimize the learning rate\n",
    "#     lr = trial.suggest_float(\"lr\", 1e-4, 1e-1, log=True)\n",
    "\n",
    "#     # Create the model\n",
    "#     model = FNOClassifier(modes=modes, channels=channels, pooling=pooling)\n",
    "\n",
    "#     # Create a learning rate scheduler and early stopping callback\n",
    "#     callbacks = [\n",
    "#         EarlyStopping(monitor=\"val_acc\", patience=20, mode=\"max\"),\n",
    "#         LearningRateMonitor(logging_interval=\"step\")\n",
    "#     ]\n",
    "\n",
    "#     # Create a tensorboard logger\n",
    "#     experiment_name = \"optuna\"\n",
    "#     logger = TensorBoardLogger(save_dir=\"logs/\", name=experiment_name, version=datetime.datetime.now().strftime(\"%Y_%m_%d-%H_%M_%S\"))\n",
    "\n",
    "#     logger.log_hyperparams({\"modes\": modes, \"lr\": lr, \"channels\": channels, \"pooling\": pooling, \"lr_scheduler\": \"ReduceLROnPlateau\", \"patience\": 10, \"min_lr\": 1e-6, \"factor\": 0.5, \"batchsize\": batch_size})\n",
    "\n",
    "#     # Create a trainer\n",
    "#     trainer = Trainer(\n",
    "#         max_epochs=100,\n",
    "#         logger=logger,\n",
    "#         callbacks=callbacks,\n",
    "#         accelerator='auto'\n",
    "#     )\n",
    "\n",
    "#     # Train the model\n",
    "#     trainer.fit(model, train_loader, valid_loader)\n",
    "\n",
    "#     # Test the model\n",
    "#     result = trainer.test(dataloaders=test_loader)\n",
    "\n",
    "#     # Return the validation accuracy\n",
    "#     return result[0][\"test_acc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# study = optuna.create_study(direction=\"maximize\")\n",
    "# study.optimize(objective, n_trials=500)\n",
    "\n",
    "# # Print the best hyperparameters\n",
    "# print(study.best_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fno",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
